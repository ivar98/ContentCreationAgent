Directory structure:
‚îî‚îÄ‚îÄ ivar98-contentcreationagent/
    ‚îú‚îÄ‚îÄ readme.md
    ‚îú‚îÄ‚îÄ app.py
    ‚îú‚îÄ‚îÄ content_graph.py
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îî‚îÄ‚îÄ utils.py

================================================
FILE: readme.md
================================================
````markdown

# ü§ñ Concept to Creation Content Assistant

This is an AI-powered creative partner that helps content creators (YouTubers, bloggers, podcasters) go from a vague idea to a fully structured content plan. The user provides a simple topic, and a team of AI agents autonomously researches, writes, critiques, and finalizes a script, learning your preferred style over time.

This project is a practical implementation of a multi-agent collaborative workflow using **LangGraph**, external tools for research, a vector database for memory, and a Streamlit UI for interaction.

## ‚ú® Features

*   **Multi-Agent Collaboration**: Utilizes a `Researcher`, `Writer`, and `Critic` agent, orchestrated by LangGraph, to mimic a real creative team.
*   **Autonomous Web Research**: The `Researcher` agent uses the Tavily Search API to gather up-to-date information on any topic.
*   **Self-Correction Loop**: The `Writer` and `Critic` agents work in a loop, refining the draft based on constructive feedback until it meets quality standards.
*   **Dual-Memory System**:
    *   **Short-Term "Research Desk"**: A ChromaDB collection stores research for the current project.
    *   **Long-Term "Style Memory"**: A persistent ChromaDB collection stores final, user-approved scripts to help the AI learn the user's preferred style.
*   **Interactive UI**: A simple and clean dashboard built with Streamlit allows users to input topics, watch the agents' progress in real-time, and view the final content plan.

## ‚öôÔ∏è Tech Stack

*   **Language**: Python
*   **Core Framework**: LangGraph, LangChain
*   **LLM**: Google Gemini Pro (via `langchain-google-genai`)
*   **Web Search Tool**: Tavily Search API
*   **Embedding Model**: `all-MiniLM-L6-v2` (from Hugging Face SentenceTransformers)
*   **Vector Database**: ChromaDB (local & persistent)
*   **Frontend**: Streamlit

## üöÄ Setup and Installation

Follow these steps to set up and run the application on your local machine.

### 1. Prerequisites

*   Python 3.9+
*   Git

### 2. Clone the Repository

```bash
git clone https://github.com/your-username/concept-to-creation.git
cd concept-to-creation
```

### 3. Set Up a Virtual Environment

It's highly recommended to use a virtual environment to manage dependencies.

```bash
# For macOS/Linux
python3 -m venv venv
source venv/bin/activate

# For Windows
python -m venv venv
.\venv\Scripts\activate
```

### 4. Install Dependencies

Install all the required Python packages from the `requirements.txt` file.

```bash
pip install -r requirements.txt
```

### 5. Configure API Keys

The application requires API keys for Google Gemini and Tavily Search.

1.  **Copy the example file**:
    ```bash
    cp .env.example .env
    ```

2.  **Edit the `.env` file**: Open the newly created `.env` file in a text editor and add your API keys.
    *   `GOOGLE_API_KEY`: Get yours from [Google AI Studio](https://aistudio.google.com/app/apikey).
    *   `TAVILY_API_KEY`: Get yours from the [Tavily API dashboard](https://app.tavily.com/).

## ‚ñ∂Ô∏è How to Run the Application

Once the setup is complete, you can start the Streamlit application with a single command:

```bash
streamlit run app.py
```

Your web browser should automatically open to the application's URL (usually `http://localhost:8501`).

## üõ†Ô∏è How It Works (Architecture)

The application's logic is orchestrated by a LangGraph state machine.

1.  **User Input**: The user provides a topic in the Streamlit UI.
2.  **Researcher**: The graph starts with the `researcher` node. This agent uses the Tavily tool to search the web for information on the topic and saves its findings.
3.  **Writer**: The `writer` node takes the research notes and queries the "Style Guide" vector DB for examples of past successful scripts. It then synthesizes this information into a first draft.
4.  **Critic**: The `critic` node reviews the draft for quality, accuracy, and structure. It provides feedback.
5.  **Conditional Edge (The Loop)**: The graph reaches a decision point.
    *   If the critic provides feedback (i.e., not "NO NOTES"), the graph routes back to the `writer` node for a revision.
    *   If the critic approves the draft ("NO NOTES") or the revision limit (2) is reached, the graph proceeds.
6.  **Finalizer**: The `finalizer` node takes the approved script and generates creative suggestions like titles and visual ideas.
7.  **Output**: The final script and creative suggestions are displayed in the UI. The user has the option to save the script to the "Style Guide" for future use.

## üìÇ Project Structure

```
concept-to-creation/
‚îú‚îÄ‚îÄ .gitignore          # Excludes unnecessary files from git
‚îú‚îÄ‚îÄ .env.example        # Template for API keys
‚îú‚îÄ‚îÄ requirements.txt    # Project dependencies
‚îú‚îÄ‚îÄ utils.py            # Handles setup of LLM, tools, and DBs
‚îú‚îÄ‚îÄ content_graph.py    # Defines the LangGraph agent workflow
‚îî‚îÄ‚îÄ app.py              # The Streamlit frontend application
```

---

## üí° Suggestions for Modifications & Improvements

The current application is a solid foundation. Here are some ways it could be extended or improved:

1.  **Add More Specialized Agents**:
    *   **`SEO_Optimizer` Agent**: An agent that takes the final script and suggests keywords, meta descriptions, and structural changes to improve search engine ranking.
    *   **`Visualizer` Agent**: An agent that generates specific prompts for image generation models (like DALL-E 3 or Midjourney) based on the script content.
    *   **`Fact-Checker` Agent**: A dedicated agent that re-runs targeted web searches to verify specific claims made in the draft, increasing the final output's reliability.

2.  **Enhance User Interaction**:
    *   **Intermediate Editing**: Allow the user to view and *edit* the draft after each critique. This would involve using Streamlit's `st.session_state` more deeply and modifying the graph to accept user overrides.
    *   **Tone & Style Selection**: Add UI elements (e.g., dropdowns, sliders) to let the user specify the desired tone (`Formal`, `Witty`, `Casual`) or content format (`Blog Post`, `YouTube Script`, `Podcast Outline`) before starting the generation. This information could be passed to the `writer`'s prompt.

3.  **More Sophisticated Memory**:
    *   **Summarized Style Profiles**: Instead of just retrieving raw scripts, create a process that periodically summarizes the "Style Guide" into a concise "style profile" document. The `writer` could use this summary for more consistent style adoption.
    *   **Knowledge Graph for Research**: For complex, multi-project topics, use a knowledge graph instead of a simple vector store for research. This would allow the agents to understand relationships between entities and build a deeper domain understanding over time.

4.  **Deployment and Scalability**:
    *   **Deploy to Streamlit Community Cloud**: The app is well-suited for deployment on Streamlit's free hosting platform.
    *   **Asynchronous Execution**: For very long content generation tasks, convert the graph execution to an asynchronous process using `graph.astream()` and manage the background task so the user doesn't have to keep the browser tab open.

5.  **Robustness and Error Handling**:
    *   Implement more explicit `try...except` blocks around all API calls (LLM, Tavily) and database interactions to handle network errors, rate limits, or API downtime gracefully, providing clearer error messages to the user.
````


================================================
FILE: app.py
================================================
import streamlit as st
from content_graph import content_creation_graph
from utils import style_guide_vector_store, check_api_keys

# --- Page Configuration ---
st.set_page_config(
    page_title="Concept to Creation Assistant",
    page_icon="ü§ñ",
    layout="wide"
)

# --- App Header ---
st.title("ü§ñ Concept to Creation Content Assistant")
st.markdown("""
Welcome! This AI-powered creative partner helps you go from a simple topic to a fully structured content plan.
Just provide a topic, and the multi-agent team will research, write, critique, and refine a script for you.
""")

st.divider()

# --- API Key Check ---
missing_keys = check_api_keys()
if missing_keys:
    st.error(f"Missing required API keys: {', '.join(missing_keys)}. Please set them in your .env file.")
    st.stop()

# --- Main Application ---
st.header("1. Start a New Content Project")

# Input field for the topic
topic = st.text_input("Enter your content topic (e.g., 'the basics of photosynthesis'):", key="topic_input")

if st.button("‚ú® Generate Content Plan", type="primary"):
    if not topic:
        st.warning("Please enter a topic to begin.")
    else:
        final_state = None
        try:
            with st.status("Agents are collaborating...", expanded=True) as status:
                graph_input = {"topic": topic}
                
                for state_update in content_creation_graph.stream(graph_input):
                    node_that_ran = list(state_update.keys())[0]
                    
                    if node_that_ran == "researcher":
                        status.update(label="üî¨ Agent 'Researcher' is gathering information...")
                    elif node_that_ran == "writer":
                        revision_num = state_update[node_that_ran].get('revision_number', 1)
                        status.update(label=f"‚úçÔ∏è Agent 'Writer' is crafting draft #{revision_num}...")
                    elif node_that_ran == "critic":
                        status.update(label="üßê Agent 'Critic' is reviewing the draft...")
                    
                    if '__end__' not in state_update:
                        final_state = state_update[node_that_ran]

                status.update(label="‚úÖ Collaboration complete!", state="complete")

        except Exception as e:
            st.error(f"An error occurred during content generation: {e}")
            st.stop()

        st.divider()
        st.header("2. Your Final Content Plan")

        if final_state:
            st.subheader("üìú Final Script")
            st.markdown(final_state.get("final_script", "No script was generated."))

            st.subheader("üí° Creative Suggestions")
            st.json(final_state.get("creative_suggestions", "No creative suggestions were generated."))
            
            st.session_state.final_script_for_saving = final_state.get("final_script")

# --- Save to Style Guide Section ---
if 'final_script_for_saving' in st.session_state and st.session_state.final_script_for_saving:
    st.divider()
    st.header("3. Improve Future Content")
    st.markdown("Saving this script to your Style Guide will help the Writer agent learn your preferred tone and structure for future projects.")

    if st.button("üëç Looks good! Save to my Style Guide"):
        if style_guide_vector_store is not None:
            try:
                style_guide_vector_store.add_texts([st.session_state.final_script_for_saving])
                st.success("Successfully saved to your Style Guide! The agents will remember this style.")
                del st.session_state.final_script_for_saving
            except Exception as e:
                st.error(f"An error occurred while saving: {e}")
        else:
            st.error("Could not save to Style Guide because the vector store is not available.")



================================================
FILE: content_graph.py
================================================
import json
from typing import TypedDict, List, Dict
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langgraph.graph import StateGraph, END

from utils import llm, tavily_tool, research_retriever, style_guide_retriever, research_vector_store

# --- 1. Define the State ---
class ContentCreationState(TypedDict):
    topic: str
    research_notes: List[str]
    draft: str
    criticism: str
    revision_number: int
    final_script: str
    creative_suggestions: Dict

# --- 2. Define the Agent Nodes ---

# Node 2.1: The Researcher Agent
research_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a world-class researcher. Your goal is to find diverse and interesting information on a given topic. You are not a writer. Your sole job is to research. Break the topic down into 3-5 sub-queries and use your search tool for each. Synthesize the results into a list of key points and facts. Do not write paragraphs or a narrative."),
    ("human", "Research the topic: {topic}"),
    # This is the crucial part you are missing!
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])
researcher_agent = create_tool_calling_agent(llm, [tavily_tool], research_prompt)
researcher_executor = AgentExecutor(agent=researcher_agent, tools=[tavily_tool], verbose=True)

def research_node(state: ContentCreationState):
    print("---NODE: RESEARCHER---")
    topic = state['topic']
    research_result = researcher_executor.invoke({"topic": topic})
    
    # Ensure research_notes is always a list
    research_notes_list = research_result['output'].split('\n') if research_result['output'] else []
    
    if research_vector_store:
        research_vector_store.add_texts(research_notes_list, metadatas=[{"source": "researcher"} for _ in research_notes_list])
    
    return {
        "research_notes": research_notes_list,
        "revision_number": 0
    }

# Node 2.2: The Writer Agent
writer_prompt_template = """
You are a professional content writer specializing in engaging scripts (for YouTube, blogs, etc.). 
Your goal is to synthesize the provided research notes into a clear, compelling, and well-structured script.

**Instructions:**
1.  **Structure:** Create a script with a clear hook, 3-4 main points, and a strong conclusion.
2.  **Tone:** Make it engaging, easy to understand, and conversational.
3.  **Use Research:** Base the script *only* on the provided research notes. Do not invent facts.
4.  **Incorporate Style (if provided):** If style examples are given, try to match their tone, structure, and voice.
5.  **Incorporate Criticism (if provided):** If criticism is provided from a previous draft, address it directly in this new version.

**Content to Use:**

**Topic:**
{topic}

**Research Notes (source of truth):**
{notes}

**Style Examples (learn from these):**
{style_examples}

**Criticism on previous draft (address this):**
{criticism}
"""
writer_prompt = ChatPromptTemplate.from_template(writer_prompt_template)
writer_runnable = writer_prompt | llm

def write_node(state: ContentCreationState):
    print(f"---NODE: WRITER (Revision {state.get('revision_number', 0)})---")
    
    style_docs = style_guide_retriever.invoke(state['topic']) if style_guide_retriever else []
    formatted_examples = "\n---\n".join([doc.page_content for doc in style_docs])
    
    formatted_notes = "\n".join(state['research_notes'])

    writer_input = {
        "topic": state['topic'],
        "notes": formatted_notes,
        "style_examples": formatted_examples,
        "criticism": state.get("criticism", "N/A")
    }
    
    draft_content = writer_runnable.invoke(writer_input)
    
    revision_num = state.get("revision_number", 0) + 1
    return {
        "draft": draft_content.content,
        "revision_number": revision_num
    }

# Node 2.3: The Critic Agent
critic_prompt_template = """
You are a sharp but fair content critic. Your job is to review a script and provide concise, actionable feedback.

**Instructions:**
1.  **Review the Draft:** Read the script below carefully.
2.  **Evaluate:** Judge it on clarity, engagement, structure, and accuracy based on the research notes.
3.  **Provide Feedback:** Give one key point of constructive feedback to improve the script. Be specific.
4.  **Approval:** If the script is excellent and needs no changes, respond with only the words: "NO NOTES".

**Content to Review:**

**Topic:**
{topic}

**Research Notes (for accuracy check):**
{notes}

**Draft Script:**
{draft}
"""
critic_prompt = ChatPromptTemplate.from_template(critic_prompt_template)
critic_runnable = critic_prompt | llm

def critic_node(state: ContentCreationState):
    print("---NODE: CRITIC---")
    
    formatted_notes = "\n".join(state['research_notes'])
    
    critic_input = {
        "topic": state['topic'],
        "notes": formatted_notes,
        "draft": state['draft']
    }
    
    criticism = critic_runnable.invoke(critic_input).content
    print(f"Critic's Feedback: {criticism}")
    
    return {"criticism": criticism}

# Node 2.4: The Finalizer Agent
finalizer_prompt_template = """
You are a creative director. The main script is complete. Your job is to provide creative suggestions to enhance the final content.

Based on the final script below, generate a JSON object with suggestions for:
1.  `title_suggestions`: 3-5 catchy titles for the content.
2.  `visual_ideas`: A list of 3-4 ideas for visuals (e.g., "An animated diagram of the Krebs cycle", "A slow-motion shot of a water droplet").
3.  `sound_effects`: A list of 2-3 sound effect ideas (e.g., "Upbeat synth music during intro", "A 'ding' sound when a key term is defined").

**Final Script:**
{final_script}
"""
finalizer_prompt = ChatPromptTemplate.from_template(finalizer_prompt_template)
finalizer_runnable = finalizer_prompt | llm

def finalize_node(state: ContentCreationState):
    print("---NODE: FINALIZER---")
    
    finalizer_input = {"final_script": state['draft']}
    creative_suggestions_raw = finalizer_runnable.invoke(finalizer_input).content

    # Robust JSON parsing
    try:
        # The model sometimes returns JSON wrapped in markdown
        json_string = creative_suggestions_raw.strip().replace("```json", "").replace("```", "")
        creative_suggestions = json.loads(json_string)
    except json.JSONDecodeError:
        creative_suggestions = {"error": "Failed to parse creative suggestions.", "raw_output": creative_suggestions_raw}

    return {
        "final_script": state['draft'],
        "creative_suggestions": creative_suggestions
    }


# --- 3. Define the Graph Edges ---
def decide_to_revise(state: ContentCreationState):
    print("---EDGE: DECISION---")
    criticism = state.get("criticism", "").strip().upper()
    revision_num = state.get("revision_number", 0)
    
    if "NO NOTES" in criticism or revision_num >= 2:
        print("Decision: Draft approved. Finalizing.")
        return "finalize"
    else:
        print("Decision: Draft needs revision.")
        return "revise"

# --- 4. Assemble the Graph ---
builder = StateGraph(ContentCreationState)

builder.add_node("researcher", research_node)
builder.add_node("writer", write_node)
builder.add_node("critic", critic_node)
builder.add_node("finalizer", finalize_node)

builder.set_entry_point("researcher")

builder.add_edge("researcher", "writer")
builder.add_edge("writer", "critic")
builder.add_conditional_edges(
    "critic",
    decide_to_revise,
    {
        "revise": "writer",
        "finalize": "finalizer"
    }
)
builder.add_edge("finalizer", END)

content_creation_graph = builder.compile()



================================================
FILE: requirements.txt
================================================
# Core LangChain Framework
# These packages provide the main orchestration and graph capabilities.
langchain
langchain-core
langchain-community
langgraph

# LLM & Tool Integrations
# For connecting to Google Gemini models.
langchain-google-genai
# For the Tavily web search tool (replaces the old langchain_community version).
langchain-tavily

# Vector Store & Embeddings
# For the ChromaDB vector store integration (replaces the old langchain_community version).
langchain-chroma
# For using local Hugging Face embedding models (replaces the old langchain_community version).
langchain-huggingface
# The underlying vector database library.
chromadb
# Powers the local embedding model (e.g., 'all-MiniLM-L6-v2').
sentence-transformers
# A required dependency for sentence-transformers and deep learning models.
torch

# Application Framework
# For building the web user interface.
streamlit

# Utilities
# For loading environment variables from a .env file.
python-dotenv


================================================
FILE: utils.py
================================================
import os
import torch
import chromadb
from dotenv import load_dotenv

# --- Updated LangChain Imports ---
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_tavily import TavilySearch  # CORRECTED IMPORT

# Load environment variables from .env file
load_dotenv()

# --- API Key Management ---
def check_api_keys():
    """
    Checks for necessary API keys in environment variables and returns a list of missing keys.
    """
    missing_keys = []
    if "TAVILY_API_KEY" not in os.environ:
        missing_keys.append("TAVILY_API_KEY")
    if "GOOGLE_API_KEY" not in os.environ:
        missing_keys.append("GOOGLE_API_KEY")
    return missing_keys

# --- Initialize LLM ---
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7, top_p=0.85)

# --- Initialize Embedding Model ---
device = 'cuda' if torch.cuda.is_available() else 'cpu'
embedding_function = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2",
    model_kwargs={'device': device}
)

# --- Initialize Vector Stores and Retrievers ---
try:
    chroma_client = chromadb.PersistentClient(path="./chroma_db")
except Exception as e:
    print(f"Error initializing ChromaDB client: {e}")
    chroma_client = None

if chroma_client:
    research_collection = chroma_client.get_or_create_collection(name="research_cache")
    research_vector_store = Chroma(
        client=chroma_client,
        collection_name="research_cache",
        embedding_function=embedding_function,
    )
    research_retriever = research_vector_store.as_retriever(search_kwargs={"k": 5})

    style_guide_collection = chroma_client.get_or_create_collection(name="style_guide")
    style_guide_vector_store = Chroma(
        client=chroma_client,
        collection_name="style_guide",
        embedding_function=embedding_function,
    )
    style_guide_retriever = style_guide_vector_store.as_retriever(search_kwargs={"k": 2})
else:
    research_retriever = None
    style_guide_retriever = None
    style_guide_vector_store = None


# --- Initialize External Tools ---
# Tavily Search is used for real-time web research.
tavily_tool = TavilySearch(max_results=5)  # CORRECTED INITIALIZATION

