Directory structure:
â””â”€â”€ ivar98-contentcreationagent/
    â”œâ”€â”€ readme.md
    â”œâ”€â”€ app.py
    â”œâ”€â”€ content_graph.py
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ utils.py

================================================
FILE: readme.md
================================================
````markdown

# ðŸ¤– Concept to Creation Content Assistant

This is an AI-powered creative partner that helps content creators (YouTubers, bloggers, podcasters) go from a vague idea to a fully structured content plan. The user provides a simple topic, and a team of AI agents autonomously researches, writes, critiques, and finalizes a script, learning your preferred style over time.

This project is a practical implementation of a multi-agent collaborative workflow using **LangGraph**, external tools for research, a vector database for memory, and a Streamlit UI for interaction.

## âœ¨ Features

*   **Multi-Agent Collaboration**: Utilizes a `Researcher`, `Writer`, and `Critic` agent, orchestrated by LangGraph, to mimic a real creative team.
*   **Autonomous Web Research**: The `Researcher` agent uses the Tavily Search API to gather up-to-date information on any topic.
*   **Self-Correction Loop**: The `Writer` and `Critic` agents work in a loop, refining the draft based on constructive feedback until it meets quality standards.
*   **Dual-Memory System**:
    *   **Short-Term "Research Desk"**: A ChromaDB collection stores research for the current project.
    *   **Long-Term "Style Memory"**: A persistent ChromaDB collection stores final, user-approved scripts to help the AI learn the user's preferred style.
*   **Interactive UI**: A simple and clean dashboard built with Streamlit allows users to input topics, watch the agents' progress in real-time, and view the final content plan.

## âš™ï¸ Tech Stack

*   **Language**: Python
*   **Core Framework**: LangGraph, LangChain
*   **LLM**: Google Gemini Pro (via `langchain-google-genai`)
*   **Web Search Tool**: Tavily Search API
*   **Embedding Model**: `all-MiniLM-L6-v2` (from Hugging Face SentenceTransformers)
*   **Vector Database**: ChromaDB (local & persistent)
*   **Frontend**: Streamlit

## ðŸš€ Setup and Installation

Follow these steps to set up and run the application on your local machine.

### 1. Prerequisites

*   Python 3.9+
*   Git

### 2. Clone the Repository

```bash
git clone https://github.com/your-username/concept-to-creation.git
cd concept-to-creation
```

### 3. Set Up a Virtual Environment

It's highly recommended to use a virtual environment to manage dependencies.

```bash
# For macOS/Linux
python3 -m venv venv
source venv/bin/activate

# For Windows
python -m venv venv
.\venv\Scripts\activate
```

### 4. Install Dependencies

Install all the required Python packages from the `requirements.txt` file.

```bash
pip install -r requirements.txt
```

### 5. Configure API Keys

The application requires API keys for Google Gemini and Tavily Search.

1.  **Copy the example file**:
    ```bash
    cp .env.example .env
    ```

2.  **Edit the `.env` file**: Open the newly created `.env` file in a text editor and add your API keys.
    *   `GOOGLE_API_KEY`: Get yours from [Google AI Studio](https://aistudio.google.com/app/apikey).
    *   `TAVILY_API_KEY`: Get yours from the [Tavily API dashboard](https://app.tavily.com/).

## â–¶ï¸ How to Run the Application

Once the setup is complete, you can start the Streamlit application with a single command:

```bash
streamlit run app.py
```

Your web browser should automatically open to the application's URL (usually `http://localhost:8501`).

## ðŸ› ï¸ How It Works (Architecture)

The application's logic is orchestrated by a LangGraph state machine.

1.  **User Input**: The user provides a topic in the Streamlit UI.
2.  **Researcher**: The graph starts with the `researcher` node. This agent uses the Tavily tool to search the web for information on the topic and saves its findings.
3.  **Writer**: The `writer` node takes the research notes and queries the "Style Guide" vector DB for examples of past successful scripts. It then synthesizes this information into a first draft.
4.  **Critic**: The `critic` node reviews the draft for quality, accuracy, and structure. It provides feedback.
5.  **Conditional Edge (The Loop)**: The graph reaches a decision point.
    *   If the critic provides feedback (i.e., not "NO NOTES"), the graph routes back to the `writer` node for a revision.
    *   If the critic approves the draft ("NO NOTES") or the revision limit (2) is reached, the graph proceeds.
6.  **Finalizer**: The `finalizer` node takes the approved script and generates creative suggestions like titles and visual ideas.
7.  **Output**: The final script and creative suggestions are displayed in the UI. The user has the option to save the script to the "Style Guide" for future use.

## ðŸ“‚ Project Structure

```
concept-to-creation/
â”œâ”€â”€ .gitignore          # Excludes unnecessary files from git
â”œâ”€â”€ .env.example        # Template for API keys
â”œâ”€â”€ requirements.txt    # Project dependencies
â”œâ”€â”€ utils.py            # Handles setup of LLM, tools, and DBs
â”œâ”€â”€ content_graph.py    # Defines the LangGraph agent workflow
â””â”€â”€ app.py              # The Streamlit frontend application
```

---

## ðŸ’¡ Suggestions for Modifications & Improvements

The current application is a solid foundation. Here are some ways it could be extended or improved:

1.  **Add More Specialized Agents**:
    *   **`SEO_Optimizer` Agent**: An agent that takes the final script and suggests keywords, meta descriptions, and structural changes to improve search engine ranking.
    *   **`Visualizer` Agent**: An agent that generates specific prompts for image generation models (like DALL-E 3 or Midjourney) based on the script content.
    *   **`Fact-Checker` Agent**: A dedicated agent that re-runs targeted web searches to verify specific claims made in the draft, increasing the final output's reliability.

2.  **Enhance User Interaction**:
    *   **Intermediate Editing**: Allow the user to view and *edit* the draft after each critique. This would involve using Streamlit's `st.session_state` more deeply and modifying the graph to accept user overrides.
    *   **Tone & Style Selection**: Add UI elements (e.g., dropdowns, sliders) to let the user specify the desired tone (`Formal`, `Witty`, `Casual`) or content format (`Blog Post`, `YouTube Script`, `Podcast Outline`) before starting the generation. This information could be passed to the `writer`'s prompt.

3.  **More Sophisticated Memory**:
    *   **Summarized Style Profiles**: Instead of just retrieving raw scripts, create a process that periodically summarizes the "Style Guide" into a concise "style profile" document. The `writer` could use this summary for more consistent style adoption.
    *   **Knowledge Graph for Research**: For complex, multi-project topics, use a knowledge graph instead of a simple vector store for research. This would allow the agents to understand relationships between entities and build a deeper domain understanding over time.

4.  **Deployment and Scalability**:
    *   **Deploy to Streamlit Community Cloud**: The app is well-suited for deployment on Streamlit's free hosting platform.
    *   **Asynchronous Execution**: For very long content generation tasks, convert the graph execution to an asynchronous process using `graph.astream()` and manage the background task so the user doesn't have to keep the browser tab open.

5.  **Robustness and Error Handling**:
    *   Implement more explicit `try...except` blocks around all API calls (LLM, Tavily) and database interactions to handle network errors, rate limits, or API downtime gracefully, providing clearer error messages to the user.
````


================================================
FILE: app.py
================================================
import streamlit as st
from content_graph import content_creation_graph
from utils import style_guide_vector_store, check_api_keys

# --- Page Configuration ---
st.set_page_config(
    page_title="Concept to Creation Assistant",
    page_icon="ðŸ¤–",
    layout="wide"
)

# --- App Header ---
st.title("ðŸ¤– Concept to Creation Content Assistant")
st.markdown("""
Welcome! This AI-powered creative partner helps you go from a simple topic to a fully structured content plan.
Just provide a topic, and the multi-agent team will research, write, critique, and refine a script for you.
""")

st.divider()

# --- API Key Check ---
missing_keys = check_api_keys()
if missing_keys:
    st.error(f"Missing required API keys: {', '.join(missing_keys)}. Please set them in your .env file.")
    st.stop()

# --- Main Application ---
st.header("1. Start a New Content Project")

# Input field for the topic
topic = st.text_input("Enter your content topic (e.g., 'the basics of photosynthesis'):", key="topic_input")

if st.button("âœ¨ Generate Content Plan", type="primary"):
    if not topic:
        st.warning("Please enter a topic to begin.")
    else:
        final_state = None
        try:
            with st.status("Agents are collaborating...", expanded=True) as status:
                graph_input = {"topic": topic}
                
                for state_update in content_creation_graph.stream(graph_input):
                    node_that_ran = list(state_update.keys())[0]
                    
                    if node_that_ran == "researcher":
                        status.update(label="ðŸ”¬ Agent 'Researcher' is gathering information...")
                    elif node_that_ran == "writer":
                        revision_num = state_update[node_that_ran].get('revision_number', 1)
                        status.update(label=f"âœï¸ Agent 'Writer' is crafting draft #{revision_num}...")
                    elif node_that_ran == "critic":
                        status.update(label="ðŸ§ Agent 'Critic' is reviewing the draft...")
                    
                    if '__end__' not in state_update:
                        final_state = state_update[node_that_ran]

                status.update(label="âœ… Collaboration complete!", state="complete")

        except Exception as e:
            st.error(f"An error occurred during content generation: {e}")
            st.stop()

        st.divider()
        st.header("2. Your Final Content Plan")

        if final_state:
            st.subheader("ðŸ“œ Final Script")
            st.markdown(final_state.get("final_script", "No script was generated."))

            st.subheader("ðŸ’¡ Creative Suggestions")
            st.json(final_state.get("creative_suggestions", "No creative suggestions were generated."))
            
            st.session_state.final_script_for_saving = final_state.get("final_script")

# --- Save to Style Guide Section ---
if 'final_script_for_saving' in st.session_state and st.session_state.final_script_for_saving:
    st.divider()
    st.header("3. Improve Future Content")
    st.markdown("Saving this script to your Style Guide will help the Writer agent learn your preferred tone and structure for future projects.")

    if st.button("ðŸ‘ Looks good! Save to my Style Guide"):
        if style_guide_vector_store is not None:
            try:
                style_guide_vector_store.add_texts([st.session_state.final_script_for_saving])
                st.success("Successfully saved to your Style Guide! The agents will remember this style.")
                del st.session_state.final_script_for_saving
            except Exception as e:
                st.error(f"An error occurred while saving: {e}")
        else:
            st.error("Could not save to Style Guide because the vector store is not available.")



================================================
FILE: content_graph.py
================================================
import json
from typing import TypedDict, List, Dict
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langgraph.graph import StateGraph, END

from utils import llm, tavily_tool, research_retriever, style_guide_retriever, research_vector_store

# --- 1. Define the State ---
class ContentCreationState(TypedDict):
    topic: str
    research_notes: List[str]
    draft: str
    criticism: str
    revision_number: int
    final_script: str
    creative_suggestions: Dict

# --- 2. Define the Agent Nodes ---

# Node 2.1: The Researcher Agent
research_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a world-class researcher. Your goal is to find diverse and interesting information on a given topic. You are not a writer. Your sole job is to research. Break the topic down into 3-5 sub-queries and use your search tool for each. Synthesize the results into a list of key points and facts. Do not write paragraphs or a narrative."),
    ("human", "Research the topic: {topic}"),
    # This is the crucial part you are missing!
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])
researcher_agent = create_tool_calling_agent(llm, [tavily_tool], research_prompt)
researcher_executor = AgentExecutor(agent=researcher_agent, tools=[tavily_tool], verbose=True)

def research_node(state: ContentCreationState):
    print("---NODE: RESEARCHER---")
    topic = state['topic']
    research_result = researcher_executor.invoke({"topic": topic})
    
    # Ensure research_notes is always a list
    research_notes_list = research_result['output'].split('\n') if research_result['output'] else []
    
    if research_vector_store:
        research_vector_store.add_texts(research_notes_list, metadatas=[{"source": "researcher"} for _ in research_notes_list])
    
    return {
        "research_notes": research_notes_list,
        "revision_number": 0
    }

# Node 2.2: The Writer Agent
writer_prompt_template = """
You are a professional content writer specializing in engaging scripts (for YouTube, blogs, etc.). 
Your goal is to synthesize the provided research notes into a clear, compelling, and well-structured script.

**Instructions:**
1.  **Structure:** Create a script with a clear hook, 3-4 main points, and a strong conclusion.
2.  **Tone:** Make it engaging, easy to understand, and conversational.
3.  **Use Research:** Base the script *only* on the provided research notes. Do not invent facts.
4.  **Incorporate Style (if provided):** If style examples are given, try to match their tone, structure, and voice.
5.  **Incorporate Criticism (if provided):** If criticism is provided from a previous draft, address it directly in this new version.

**Content to Use:**

**Topic:**
{topic}

**Research Notes (source of truth):**
{notes}

**Style Examples (learn from these):**
{style_examples}

**Criticism on previous draft (address this):**
{criticism}
"""
writer_prompt = ChatPromptTemplate.from_template(writer_prompt_template)
writer_runnable = writer_prompt | llm

def write_node(state: ContentCreationState):
    print(f"---NODE: WRITER (Revision {state.get('revision_number', 0)})---")
    
    style_docs = style_guide_retriever.invoke(state['topic']) if style_guide_retriever else []
    formatted_examples = "\n---\n".join([doc.page_content for doc in style_docs])
    
    formatted_notes = "\n".join(state['research_notes'])

    writer_input = {
        "topic": state['topic'],
        "notes": formatted_notes,
        "style_examples": formatted_examples,
        "criticism": state.get("criticism", "N/A")
    }
    
    draft_content = writer_runnable.invoke(writer_input)
    
    revision_num = state.get("revision_number", 0) + 1
    return {
        "draft": draft_content.content,
        "revision_number": revision_num
    }

# Node 2.3: The Critic Agent
critic_prompt_template = """
You are a sharp but fair content critic. Your job is to review a script and provide concise, actionable feedback.

**Instructions:**
1.  **Review the Draft:** Read the script below carefully.
2.  **Evaluate:** Judge it on clarity, engagement, structure, and accuracy based on the research notes.
3.  **Provide Feedback:** Give one key point of constructive feedback to improve the script. Be specific.
4.  **Approval:** If the script is excellent and needs no changes, respond with only the words: "NO NOTES".

**Content to Review:**

**Topic:**
{topic}

**Research Notes (for accuracy check):**
{notes}

**Draft Script:**
{draft}
"""
critic_prompt = ChatPromptTemplate.from_template(critic_prompt_template)
critic_runnable = critic_prompt | llm

def critic_node(state: ContentCreationState):
    print("---NODE: CRITIC---")
    
    formatted_notes = "\n".join(state['research_notes'])
    
    critic_input = {
        "topic": state['topic'],
        "notes": formatted_notes,
        "draft": state['draft']
    }
    
    criticism = critic_runnable.invoke(critic_input).content
    print(f"Critic's Feedback: {criticism}")
    
    return {"criticism": criticism}

# Node 2.4: The Finalizer Agent
finalizer_prompt_template = """
You are a creative director. The main script is complete. Your job is to provide creative suggestions to enhance the final content.

Based on the final script below, generate a JSON object with suggestions for:
1.  `title_suggestions`: 3-5 catchy titles for the content.
2.  `visual_ideas`: A list of 3-4 ideas for visuals (e.g., "An animated diagram of the Krebs cycle", "A slow-motion shot of a water droplet").
3.  `sound_effects`: A list of 2-3 sound effect ideas (e.g., "Upbeat synth music during intro", "A 'ding' sound when a key term is defined").

**Final Script:**
{final_script}
"""
finalizer_prompt = ChatPromptTemplate.from_template(finalizer_prompt_template)
finalizer_runnable = finalizer_prompt | llm

def finalize_node(state: ContentCreationState):
    print("---NODE: FINALIZER---")
    
    finalizer_input = {"final_script": state['draft']}
    creative_suggestions_raw = finalizer_runnable.invoke(finalizer_input).content

    # Robust JSON parsing
    try:
        # The model sometimes returns JSON wrapped in markdown
        json_string = creative_suggestions_raw.strip().replace("```json", "").replace("```", "")
        creative_suggestions = json.loads(json_string)
    except json.JSONDecodeError:
        creative_suggestions = {"error": "Failed to parse creative suggestions.", "raw_output": creative_suggestions_raw}

    return {
        "final_script": state['draft'],
        "creative_suggestions": creative_suggestions
    }


# --- 3. Define the Graph Edges ---
def decide_to_revise(state: ContentCreationState):
    print("---EDGE: DECISION---")
    criticism = state.get("criticism", "").strip().upper()
    revision_num = state.get("revision_number", 0)
    
    if "NO NOTES" in criticism or revision_num >= 2:
        print("Decision: Draft approved. Finalizing.")
        return "finalize"
    else:
        print("Decision: Draft needs revision.")
        return "revise"

# --- 4. Assemble the Graph ---
builder = StateGraph(ContentCreationState)

builder.add_node("researcher", research_node)
builder.add_node("writer", write_node)
builder.add_node("critic", critic_node)
builder.add_node("finalizer", finalize_node)

builder.set_entry_point("researcher")

builder.add_edge("researcher", "writer")
builder.add_edge("writer", "critic")
builder.add_conditional_edges(
    "critic",
    decide_to_revise,
    {
        "revise": "writer",
        "finalize": "finalizer"
    }
)
builder.add_edge("finalizer", END)

content_creation_graph = builder.compile()



================================================
FILE: requirements.txt
================================================
# Core LangChain Framework
# These packages provide the main orchestration and graph capabilities.
langchain
langchain-core
langchain-community
langgraph

# LLM & Tool Integrations
# For connecting to Google Gemini models.
langchain-google-genai
# For the Tavily web search tool (replaces the old langchain_community version).
langchain-tavily

# Vector Store & Embeddings
# For the ChromaDB vector store integration (replaces the old langchain_community version).
langchain-chroma
# For using local Hugging Face embedding models (replaces the old langchain_community version).
langchain-huggingface
# The underlying vector database library.
chromadb
# Powers the local embedding model (e.g., 'all-MiniLM-L6-v2').
sentence-transformers
# A required dependency for sentence-transformers and deep learning models.
torch

# Application Framework
# For building the web user interface.
streamlit

# Utilities
# For loading environment variables from a .env file.
python-dotenv


================================================
FILE: utils.py
================================================
import os
import torch
import chromadb
from dotenv import load_dotenv

# --- Updated LangChain Imports ---
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_tavily import TavilySearch  # CORRECTED IMPORT

# Load environment variables from .env file
load_dotenv()

# --- API Key Management ---
def check_api_keys():
    """
    Checks for necessary API keys in environment variables and returns a list of missing keys.
    """
    missing_keys = []
    if "TAVILY_API_KEY" not in os.environ:
        missing_keys.append("TAVILY_API_KEY")
    if "GOOGLE_API_KEY" not in os.environ:
        missing_keys.append("GOOGLE_API_KEY")
    return missing_keys

# --- Initialize LLM ---
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7, top_p=0.85)

# --- Initialize Embedding Model ---
device = 'cuda' if torch.cuda.is_available() else 'cpu'
embedding_function = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2",
    model_kwargs={'device': device}
)

# --- Initialize Vector Stores and Retrievers ---
try:
    chroma_client = chromadb.PersistentClient(path="./chroma_db")
except Exception as e:
    print(f"Error initializing ChromaDB client: {e}")
    chroma_client = None

if chroma_client:
    research_collection = chroma_client.get_or_create_collection(name="research_cache")
    research_vector_store = Chroma(
        client=chroma_client,
        collection_name="research_cache",
        embedding_function=embedding_function,
    )
    research_retriever = research_vector_store.as_retriever(search_kwargs={"k": 5})

    style_guide_collection = chroma_client.get_or_create_collection(name="style_guide")
    style_guide_vector_store = Chroma(
        client=chroma_client,
        collection_name="style_guide",
        embedding_function=embedding_function,
    )
    style_guide_retriever = style_guide_vector_store.as_retriever(search_kwargs={"k": 2})
else:
    research_retriever = None
    style_guide_retriever = None
    style_guide_vector_store = None


# --- Initialize External Tools ---
# Tavily Search is used for real-time web research.
tavily_tool = TavilySearch(max_results=5)  # CORRECTED INITIALIZATION

